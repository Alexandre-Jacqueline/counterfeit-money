{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression logistique "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous souhaitons ici modéliser, classfier, \"is_genuine\",une variable binaire [0,1] en fonction des variables explicatives quantitatives \n",
    "(et potentiellement quqlitatives) à l'aide d'une regression logistique.\n",
    "La regression logistique est une méthode de classification supervisée.\n",
    "\n",
    "L'objectif ici est de modéliser la probabilité d'obtenir des vrai ou faux billet de banque en fonction des constantes des billets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable is_genuine, qui prend comme valeurs 1 ou  0, suit une loi de Bernoulli de paramètre p(x) dépendant de x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Machine Learning consiste à nourrir un algorithme à l’aide de données afin qu’il apprenne par lui-même à effectuer une certaine tâche. Dans les problèmes de classification, il prédit des résultats que l’on doit comparer à la réalité pour mesurer son degré de performance. On utilise généralement la matrice de confusion, appelée aussi tableau de contingence. Elle mettra non seulement en valeur les prédictions correctes et incorrectes mais nous donnera surtout un indice sur le type d’erreurs commises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "\n",
    "import os\n",
    "os.chdir('/Users/alexandrejacqueline/documents/scriptopc/P6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_genuine</th>\n",
       "      <th>diagonal</th>\n",
       "      <th>height_left</th>\n",
       "      <th>height_right</th>\n",
       "      <th>margin_low</th>\n",
       "      <th>margin_up</th>\n",
       "      <th>length</th>\n",
       "      <th>Clusters_KM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>171.81</td>\n",
       "      <td>104.86</td>\n",
       "      <td>104.95</td>\n",
       "      <td>4.52</td>\n",
       "      <td>2.89</td>\n",
       "      <td>112.83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_genuine  diagonal  height_left  height_right  margin_low  margin_up  \\\n",
       "0           1    171.81       104.86        104.95        4.52       2.89   \n",
       "\n",
       "   length  Clusters_KM  \n",
       "0  112.83            1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('mypicklefile_P6.pkl','rb') as f1:\n",
    "    my_unpickler = pickle.Unpickler(f1)\n",
    "    notes = my_unpickler.load()\n",
    "notes.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notes = pd.read_csv('notes.csv')\n",
    "notes['is_genuine'] = notes['is_genuine'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression logistique avec SciKit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir notre variable dépendante y et nos varaibles indépendantes X\n",
    "X = notes.drop([\"is_genuine\",\"Clusters_KM\"],axis = 1)\n",
    "y = notes.iloc[:, 0].values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation croisée "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser le dataset entre le Training set et le Test set\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "#Donnée standardisées\n",
    "X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(X, y, test_size = 0.25, random_state = 0) #Standard Scaler\n",
    "\n",
    "#Données originelles\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reg_Log_0_Test\n",
    "# Feature Scaling, mis ç l'échelle des données \n",
    "# Pour modèle type clustering ou notion de distance tjrs necessaire le centrage/reduction pour eviter ce problème d'échelle, histoire d'ordre de grandeur,peu importe l'unité ...\n",
    "\n",
    "sc = StandardScaler()  #Permet de centrer reduire les variables        \n",
    "X_train_0 = sc.fit_transform(X_train_0)\n",
    "X_test_0 = sc.transform(X_test_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction et entrainement de nos modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 161 ms, sys: 31.7 ms, total: 193 ms\n",
      "Wall time: 338 ms\n",
      "CPU times: user 10.4 ms, sys: 1.11 ms, total: 11.5 ms\n",
      "Wall time: 11.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Construction du modèle\n",
    "classifier = LogisticRegression() #Création du modèle \n",
    "\n",
    "#Entrainement du modèle 0 Donnée standardisées\n",
    "%time classifier.fit(X_train_0, y_train_0)\n",
    "\n",
    "#Entrainement du modèle Données originelles\n",
    "%time classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le score de prédiction est de 98% de taux de fiabilité\n"
     ]
    }
   ],
   "source": [
    "# Faire de nouvelles prédictions, résultat des prédiction du x_test_0   / Coef de determination \n",
    "y_pred_0 = classifier.predict(X_test_0)\n",
    "score=classifier.score(X_test_0,y_test_0)\n",
    "print(\"Le score de prédiction est de {}% de taux de fiabilité\".format(round(score*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le score de prédiction est de 100% de taux de fiabilité\n"
     ]
    }
   ],
   "source": [
    "# Faire de nouvelles prédictions, résultat des prédiction du x_test\n",
    "y_pred = classifier.predict(X_test)\n",
    "score=classifier.score(X_test,y_test)\n",
    "print(\"Le score de prédiction est de {}% de taux de fiabilité\".format(round(score*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La standartisation ici crée une perte de performance de notre modèle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrice de confusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La performance d’un algorithme de Machine Learning est directement liée à sa capacité à prédire un résultat . Lorsque l’on cherche à comparer les résultats d’un algorithme à la réalité , on utilise une matrice de confusion ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  0]\n",
      " [ 0 25]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre régle de décision est parfaite car les nombres de faux positifs et de faux négatifs sont nuls.(En pratique ce n'est jamais le cas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        18\n",
      "           1       1.00      1.00      1.00        25\n",
      "\n",
      "    accuracy                           1.00        43\n",
      "   macro avg       1.00      1.00      1.00        43\n",
      "weighted avg       1.00      1.00      1.00        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En comparaison avec notre modèle 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        18\n",
      "           1       1.00      0.96      0.98        25\n",
      "\n",
      "    accuracy                           0.98        43\n",
      "   macro avg       0.97      0.98      0.98        43\n",
      "weighted avg       0.98      0.98      0.98        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_0, y_pred_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found that accuracy of the model is 100 %. By accuracy, we mean the number of correct predictions divided by the total number of predictions.\n"
     ]
    }
   ],
   "source": [
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "accuracy_percentage = 100 * accuracy\n",
    "print('We found that accuracy of the model is',round(accuracy_percentage),'%. By accuracy, we mean the number of correct predictions divided by the total number of predictions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://eric.univ-lyon2.fr/~ricco/cours/cours_econometrie.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classifier_pred.pkl', 'wb') as f1:\n",
    "    my_pickler = pickle.Pickler(f1)\n",
    "    my_pickler.dump(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mypicklefile_P6_P2.pkl', 'wb') as f1:\n",
    "    my_pickler = pickle.Pickler(f1)\n",
    "    my_pickler.dump(notes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
